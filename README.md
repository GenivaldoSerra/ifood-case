## **Requisitos:**
* Fazer fork desse projeto
* Conta AWS
* Conta Databricks Free Edition

* Clonar reposit√≥rio no Databricks (validar se tem endpoint no Databricks API para isso)
  Endpoint: https://docs.databricks.com/api/workspace/repos/create
  * How to: (sum√°rio)
* Par√¢metros/Credenciais a criar:
  * AWS Console:
    * Access Key (Chave de Acesso)
    * Instance Provider (Provedor de Inst√¢ncia)
    * Role para o Instance Provider com o nome `NycTripRecordOidcRole` 
      > IMPORTANTE: √â necess√°rio usar esse nome para funcionamento da cria√ß√£o do S3
  * Databricks:
    * Personal Access Token (PAT) do Databricks
  * Github:
    * Repository Secrets:
      * AWS_ACCESS_KEY_ID: Com esse nome, e valor da access key da AWS criada
      * AWS_SECRET_ACCESS_KEY: Com esse nome, e valor da access key secret da AWS criada
      * DATABRICKS_TOKEN: Com esse nome, e valor do PAT Databricks
      * NYC_TRIP_RECORD_OIDC_ROLE_ARN: Com esse nome, e valor do ARN da Role do Instance Provider criada
* Par√¢metros a informar:
  * Workflow Databricks Setup:
    * databricks_host: Url do Databricks antes do par√¢metro "?o=<n√∫mero_workspace>
      * Ex: url: https://dbc-ab3dba61-89cc.cloud.databricks.com/?o=3912183202474156; databricks_host: https://dbc-ab3dba61-89cc.cloud.databricks.com/
    * databricks_user_email: Seu email utilizado para login no Workspace Databricks
  * Workflow NYC Trip Record S3 Setup:
    * environment: Com os valores 'dev' ou 'prod' (a ser implementado)
    * aws_region: Com o valor da regi√£o a criar o bucket S3
      > IMPORTANTE: Para a vers√£o free do Databricks, usar a regi√£o us-east-2 para funcionamento da integra√ß√£o AWS <> Databricks
* Configurar conex√£o Github<>Terraform:
  * Criar OIDC (OpenID Connect):
    * Refer√™ncia [aqui](https://aws.amazon.com/pt/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/)
    * Necess√°rio:
      * Informar nome do seu perfil no campo `Github Organization` , na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
      * Coloque `NycTripRecordOidcRole` como nome da role na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
      * Selecione a role: `AmazonS3FullAccess` na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
    * Recomendado:
      * Informar url do reposit√≥rio que foi feito o fork, na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima
      * Colocar tags desse projeto;
      * Especificar branch do reposit√≥rio que foi feito o fork, na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima
  * Adicionar `ARN` do Identity Provider criado nas vari√°veis do reposit√≥rio criado
    * No seu reposit√≥rio acesse:
      * Aba `Settings` 
      * Na se√ß√£o `Security`, clique em `Secrets and variables` 
      *Clique em `Actions` 
      * Na sequ√™ncia, clique na aba `Variables` 
      * Crie uma vari√°vel com o nome `NYC_TRIP_RECORD_OIDC_ARN`, com o valor do ARN do Identity Provider criado
      * Crie uma vari√°vel com o nome `NYC_TRIP_RECORD_AWS_REGION` com o valor us-east-2
    * **IMPORTANTE:** os nomes acima e a cria√ß√£o dessas vari√°veis √© necess√°rio para deploy do S3 que utiliza essas configura√ß√µes

* Configurar conex√£o AWS<>Databricks Free Edition 
  * Limita√ß√µes (para ADR): Instance profile precisa de databricks provisionado na AWS: https://docs.databricks.com/aws/pt/connect/storage/tutorial-s3-instance-profile
  * Storage credential e external location tem restri√ß√µes tambem. Cria√ß√£o delas foi feita com sucesso atrav√©s do AWS Quickstart (que utiliza CloudFormation)
  * Decis√£o: criar access_key e access_secret_key no Console AWS e colocar em secrets github

### **Passo 1 ‚Äì Entrar no IAM**

1. Acesse o console da AWS.
2. V√° em **IAM ‚Üí Users**.
3. Clique no usu√°rio que voc√™ quer usar (ex: `nyc-trip-record-databricks`).

   > Se n√£o existir, crie o usu√°rio primeiro:
   >
   > * User name: `nyc-trip-record-databricks`
   > * Access type: **Programmatic access** (necess√°rio para boto3/CLI)
   > * N√£o esque√ßa de adicionar ao menos permiss√µes para o bucket S3 que voc√™ vai usar.



### **Passo 2 ‚Äì Criar Access Key**

1. Dentro do usu√°rio, clique na aba **Security credentials**.
2. Role at√© **Access keys**.
3. Clique em **Create access key**.
4. Escolha **Programmatic access** e confirme.
5. Voc√™ ver√° **Access Key ID** e **Secret Access Key** ‚Äî **salve em local seguro**, pois o Secret s√≥ aparece uma vez.

---

### **Passo 3 ‚Äì Salvar no GitHub Secrets**

No reposit√≥rio do GitHub:

1. V√° em **Settings ‚Üí Secrets and variables ‚Üí Actions ‚Üí New repository secret**.
2. Crie dois secrets:

   * `AWS_ACCESS_KEY_ID` ‚Üí cole o Access Key ID
   * `AWS_SECRET_ACCESS_KEY` ‚Üí cole o Secret Access Key


* Criar external location:

Boa, Calili üöÄ j√° temos os dados no bucket üéâ Agora vamos organizar o **cat√°logo e schemas** no Databricks para consumir esses arquivos como tabelas externas. Vou te enumerar os passos do fluxo completo:

---

### **1. Criar a External Location**

Na **free edition** do Databricks, voc√™ provavelmente n√£o conseguir√° rodar o comando `CREATE EXTERNAL LOCATION` direto no SQL, porque isso exige Unity Catalog + admin.
üëâ Alternativas:

* **(A)** Criar via **AWS Quickstart** (ele provisiona a role + external location automaticamente).
* **(B)** Se n√£o der, voc√™ pode **usar apenas credenciais via secrets** (como j√° fez) e acessar o bucket diretamente no Spark.

Mas o ideal (se tiver suporte UC ativado) seria:

```sql
CREATE EXTERNAL LOCATION nyc_trip_location
URL 's3://nyc-trip-record-ifood/raw'
WITH (STORAGE CREDENTIAL aws_credential);
```

Onde `aws_credential` √© configurado via:

```sql
CREATE STORAGE CREDENTIAL aws_credential
WITH AWS_ACCESS_KEY_ID = '{{seu_access_key}}'
AWS_SECRET_ACCESS_KEY = '{{seu_secret_key}}';
```

‚ö†Ô∏è No **free**, isso provavelmente n√£o vai rolar ‚Äî ent√£o voc√™ usar√° `dbutils.fs.mount` ou apenas path direto `s3://...`.

---

### **2. Criar o Cat√°logo**

Mesmo sem external location, voc√™ consegue criar o cat√°logo para organizar os schemas.
O cat√°logo precisa de uma pasta managed (mas pode ser s√≥ a raiz do bucket, j√° que voc√™ n√£o vai usar tabelas managed agora).

```sql
CREATE CATALOG nyc_trip_catalog
MANAGED LOCATION 's3://nyc-trip-record-ifood/';
```

---

### **3. Criar os Schemas**

Criamos tr√™s camadas (`raw`, `trusted`, `refined`).
No caso de external tables, vamos associar cada schema a uma pasta:

```sql
CREATE SCHEMA nyc_trip_catalog.raw
MANAGED LOCATION 's3://nyc-trip-record-ifood/raw';

CREATE SCHEMA nyc_trip_catalog.trusted
MANAGED LOCATION 's3://nyc-trip-record-ifood/trusted';

CREATE SCHEMA nyc_trip_catalog.refined
MANAGED LOCATION 's3://nyc-trip-record-ifood/refined';
```

---

### **4. Criar as Tabelas Externas (por cor de t√°xi)**

Como voc√™ j√° organizou os arquivos em pastas (`fhv`, `fhvhv`, `green`, `yellow`), d√° pra criar uma tabela para cada.
No SQL:

```sql
USE CATALOG nyc_trip_catalog;
USE SCHEMA raw;

-- Tabela FHV
CREATE TABLE fhv
USING PARQUET
LOCATION 's3://nyc-trip-record-ifood/raw/fhv/';

-- Tabela FHVHV
CREATE TABLE fhvhv
USING PARQUET
LOCATION 's3://nyc-trip-record-ifood/raw/fhvhv/';

-- Tabela Green
CREATE TABLE green
USING PARQUET
LOCATION 's3://nyc-trip-record-ifood/raw/green/';

-- Tabela Yellow
CREATE TABLE yellow
USING PARQUET
LOCATION 's3://nyc-trip-record-ifood/raw/yellow/';
```

---

### **5. Testar as Consultas**

Depois √© s√≥ rodar no SQL do Databricks:

```sql
SELECT COUNT(*) FROM nyc_trip_catalog.raw.green;
SELECT * FROM nyc_trip_catalog.raw.yellow LIMIT 10;
```

---

### **Resumo dos passos na ordem**

1. (Opcional) Criar External Location via Quickstart (se conseguir).
2. Criar cat√°logo apontando para o bucket raiz.
3. Criar schemas (`raw`, `trusted`, `refined`) apontando para subpastas.
4. Criar tabelas externas por cor de t√°xi (`fhv`, `fhvhv`, `green`, `yellow`).
5. Testar queries para validar acesso.

---

üëâ Pergunta pra ti:
Quer que eu j√° monte **uma vers√£o ‚Äúplano B‚Äù sem Unity Catalog**, s√≥ com `spark.read.parquet("s3://...")` e depois `saveAsTable`? Assim voc√™ n√£o depende do `CREATE EXTERNAL LOCATION`.
