# Boas vindas ao **ifood-case!**

Para executar o projeto, observe as orienta√ß√µes descritas a seguir, e se tiver qualquer d√∫vida, sugest√£o, contribui√ß√£o, considere abrir uma issue ou entrar em contato. üöÄ

Aqui voc√™ vai encontrar os detalhes de como est√° estruturado e foi desenvolvido o projeto.

# <a id='topicos'>T√≥picos</a>
- [Desenvolvimento](#desenvolvimento)
  - [Objetivo](#objetivo)
  - [Estrutura do projeto](#estrutura)
  - [Tecnologias utilizadas](#tecnologias)
- [Orienta√ß√µes](#orientacoes)
  - [Executando o projeto](#execucao)
- [Implementa√ß√µes](#implementacoes)
  - [Contextualizando](#contextualizando)
  - [Continuous Delivery](#ci)
    - [NYC Bucket Setup](#bs)
    - [Databricks Setup](#db)
  - [Camada de Consumo](#cl)
    - [Desenho do ambiente](#layers)
    - [Modelagem de Dados](#der)
- [Decis√µes Arquiteturais](#adr)
  - [Defini√ß√µes de Solu√ß√£o](#c4-model)
  - [Registros de Decis√£o](#registros)
- [Pr√≥ximos passos](#next)

# <a id='desenvolvimento'>[Desenvolvimento](#topicos)</a>

<strong><a id='objetivo'>[Objetivo](#topicos)</a></strong>

  O **objetivo** √© construir um datalake com os dados da Taxi & Limousine Comission (TLC) de Nova York, dispon√≠veis [aqui](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Em um primeiro recorte, com o sample de Janeiro a Maio de 2023.
  
  Para isso, foi definida uma [**arquitetura de refer√™ncia**](#c4-model) e [**modelagem dos dados**](#der) tornando a disponibilidade e consumo do ambiente anal√≠tico escal√°vel e resiliente.

  ---

<strong><a id='estrutura'>[Estrutura do projeto](#topicos)</a></strong>

* **Na pasta [.github](.github) est√£o os diret√≥rios:**
  * **[actions](.github\actions)** com custom actions do `GitHub Actions`, modularizando e desaclopando steps dos workflows de subida dos componentes da arquitetura de refer√™ncia;
  * **[workflows](.github\workflows)** com os workflows `GitHub Actions` que iniciam o setup das duas solu√ß√µes adotadas para constru√ß√£o do datalake para o projeto:
    * **[databricks_setup](.github\workflows\databricks_setup.yml)** que adiciona as secrets, esse reposit√≥rio e cria no ambiente Databricks informado, os jobs de ingest√£o e transforma√ß√£o de dados da TLC;
    * **[nyc_bucket_setup](.github\workflows\nyc_bucket_setup.yml)** que cria um bucket S3 no console AWS informado, para storage do datalake pavimentado pelo Databricks;
* **Na pasta [analysis](analysis) est√£o os arquivos que endere√ßam quest√µes de neg√≥cio sobre os dados do TLC NYC**;
* **Na pasta [devops](devops) est√£o os m√≥dulos utilizados pelos workflows `GitHub Actions`** para pavimenta√ß√£o do Databricks e S3 (baseado em Terraform);
* **Na pasta [src](src) est√£o os diret√≥rios:**
  * **[dataops](src\dataops)** com os c√≥digos fonte utilizados para as opera√ß√µes do ambiente anal√≠tico, atualmente no processamento de dados espec√≠ficos da origem que n√£o puderam seguir o fluxo normal. Mais sobre o tema [aqui](#registros);
  * **[ingestion](src\ingestion)** com os c√≥digos fonte do job de ingest√£o dos dados do TLC para a camada bruta (raw) de processamento;
  * **[transform](src\transform)** com os c√≥digos fonte do job de transforma√ß√µes dos dados brutos e pouso nas camadas de consumo dos times de an√°lise (refined e trusted);

  ---

<strong><a id='tecnologias'>[Tecnologias utilizadas](#topicos)</a></strong>

  O projeto foi desenvolvido utilizando o AWS S3 como solu√ß√£o de armazenamento dos dados, e o Databricks Free Edition como solu√ß√£o para aquisi√ß√£o, processamento e consumo (serving) no ambiente anal√≠tico.

  Para provisionamento do c√≥digo, o `Github Actions` foi a solu√ß√£o utilizada para entrega e integra√ß√£o cont√≠nua da infraestrutura e desenvolvimento do ambiente. E o `Terraform` foi a op√ß√£o empregada para versionamento e deploy da infraestrutura integral do S3.

  Ainda sobre o processamento dos dados, o `Python` foi utilizado como API principal, e as bibliotecas e engines utilizadas atrav√©s do Databricks, baseadas no Python, foram as abaixo

  * **[boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html):** Kit de desenvolvimento (SDK) da AWS que habilita via c√≥digo ter uma interface program√°tica com os recursos da cloud. Aqui utilizada como solu√ß√£o de aquisi√ß√£o de dados, dada a integra√ß√£o da ferramenta abaixo com processamento dos recursos
  * **[PySpark](https://spark.apache.org/docs/latest/api/python/index.html):** API em python do `Apache Spark`, que habilita atrav√©s da linguagem o processamento de dados massivos (Big Data). √â tamb√©m solu√ß√£o prim√°ria no Databricks, que opera via Cluster Jobs do Spark toda a carga e recursos de rede utilizados no Databricks.

  ---

# <a id='orientacoes'>[Orienta√ß√µes](#topicos)</a>

<strong><a id='execucao'>[Executando o projeto](#topicos)</a></strong>


  ---

# <a id='implementacoes'>[Implementa√ß√µes](#topicos)</a>

<strong><a id='contextualizando'>[Contextualizando](#topicos)</a></strong>

<strong><a id='ci'>[Continuous Delivery](#topicos)</a></strong>

<strong><a id='bs'>[NYC Bucket Setup](#topicos)</a></strong>

<strong><a id='db'>[Databricks Setup](#topicos)</a></strong>

<strong><a id='cl'>[Camada de Consumo](#topicos)</a></strong>

<strong><a id='layers'>[Desenho do ambiente](#topicos)</a></strong>

<strong><a id='der'>[Modelagem de Dados](#topicos)</a></strong>


  ---

# <a id='adr'>[Decis√µes Arquiteturais](#topicos)</a>

<strong><a id='c4-model'>[Defini√ß√µes de Solu√ß√£o](#topicos)</a></strong>

<strong><a id='registros'>[Registros de Decis√£o](#topicos)</a></strong>


  ---

# <a id='next'>[Pr√≥ximos passos](#topicos)</a>

## **Requisitos:**
* Fazer fork desse projeto
* Conta AWS
* Conta Databricks Free Edition

* Clonar reposit√≥rio no Databricks (validar se tem endpoint no Databricks API para isso)
  Endpoint: https://docs.databricks.com/api/workspace/repos/create
  * How to: (sum√°rio)
* Par√¢metros/Credenciais a criar:
  * AWS Console:
    * Access Key (Chave de Acesso)
    * Instance Provider (Provedor de Inst√¢ncia)
    * Role para o Instance Provider com o nome `NycTripRecordOidcRole` 
      > IMPORTANTE: √â necess√°rio usar esse nome para funcionamento da cria√ß√£o do S3
  * Databricks:
    * Personal Access Token (PAT) do Databricks
  * Github:
    * Repository Secrets:
      * AWS_ACCESS_KEY_ID: Com esse nome, e valor da access key da AWS criada
      * AWS_SECRET_ACCESS_KEY: Com esse nome, e valor da access key secret da AWS criada
      * DATABRICKS_TOKEN: Com esse nome, e valor do PAT Databricks
      * NYC_TRIP_RECORD_OIDC_ROLE_ARN: Com esse nome, e valor do ARN da Role do Instance Provider criada
* Par√¢metros a informar:
  * Workflow Databricks Setup:
    * databricks_host: Url do Databricks antes do par√¢metro "?o=<n√∫mero_workspace>
      * Ex: url: https://dbc-ab3dba61-89cc.cloud.databricks.com/?o=3912183202474156; databricks_host: https://dbc-ab3dba61-89cc.cloud.databricks.com/
    * databricks_user_email: Seu email utilizado para login no Workspace Databricks
  * Workflow NYC Trip Record S3 Setup:
    * environment: Com os valores 'dev' ou 'prod' (a ser implementado)
    * aws_region: Com o valor da regi√£o a criar o bucket S3
      > IMPORTANTE: Para a vers√£o free do Databricks, usar a regi√£o us-east-2 para funcionamento da integra√ß√£o AWS <> Databricks
* Configurar conex√£o Github<>Terraform:
  * Criar OIDC (OpenID Connect):
    * Refer√™ncia [aqui](https://aws.amazon.com/pt/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/)
    * Necess√°rio:
      * Informar nome do seu perfil no campo `Github Organization` , na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
      * Coloque `NycTripRecordOidcRole` como nome da role na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
      * Selecione a role: `AmazonS3FullAccess` na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
    * Recomendado:
      * Informar url do reposit√≥rio que foi feito o fork, na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima
      * Colocar tags desse projeto;
      * Especificar branch do reposit√≥rio que foi feito o fork, na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima
  * Adicionar `ARN` do Identity Provider criado nas vari√°veis do reposit√≥rio criado
    * No seu reposit√≥rio acesse:
      * Aba `Settings` 
      * Na se√ß√£o `Security`, clique em `Secrets and variables` 
      *Clique em `Actions` 
      * Na sequ√™ncia, clique na aba `Variables` 
      * Crie uma vari√°vel com o nome `NYC_TRIP_RECORD_OIDC_ARN`, com o valor do ARN do Identity Provider criado
      * Crie uma vari√°vel com o nome `NYC_TRIP_RECORD_AWS_REGION` com o valor us-east-2
    * **IMPORTANTE:** os nomes acima e a cria√ß√£o dessas vari√°veis √© necess√°rio para deploy do S3 que utiliza essas configura√ß√µes

* Configurar conex√£o AWS<>Databricks Free Edition 
  * Limita√ß√µes (para ADR): Instance profile precisa de databricks provisionado na AWS: https://docs.databricks.com/aws/pt/connect/storage/tutorial-s3-instance-profile
  * Storage credential e external location tem restri√ß√µes tambem. Cria√ß√£o delas foi feita com sucesso atrav√©s do AWS Quickstart (que utiliza CloudFormation)
  * Decis√£o: criar access_key e access_secret_key no Console AWS e colocar em secrets github

### **Passo 1 ‚Äì Entrar no IAM**

1. Acesse o console da AWS.
2. V√° em **IAM ‚Üí Users**.
3. Clique no usu√°rio que voc√™ quer usar (ex: `nyc-trip-record-databricks`).

   > Se n√£o existir, crie o usu√°rio primeiro:
   >
   > * User name: `nyc-trip-record-databricks`
   > * Access type: **Programmatic access** (necess√°rio para boto3/CLI)
   > * N√£o esque√ßa de adicionar ao menos permiss√µes para o bucket S3 que voc√™ vai usar.



### **Passo 2 ‚Äì Criar Access Key**

1. Dentro do usu√°rio, clique na aba **Security credentials**.
2. Role at√© **Access keys**.
3. Clique em **Create access key**.
4. Escolha **Programmatic access** e confirme.
5. Voc√™ ver√° **Access Key ID** e **Secret Access Key** ‚Äî **salve em local seguro**, pois o Secret s√≥ aparece uma vez.

---

### **Passo 3 ‚Äì Salvar no GitHub Secrets**

No reposit√≥rio do GitHub:

1. V√° em **Settings ‚Üí Secrets and variables ‚Üí Actions ‚Üí New repository secret**.
2. Crie dois secrets:

   * `AWS_ACCESS_KEY_ID` ‚Üí cole o Access Key ID
   * `AWS_SECRET_ACCESS_KEY` ‚Üí cole o Secret Access Key


* Configurar ingest√£o
  * Documentar CloudFormation
  * Execu√ß√£o dos jobs de ingest√£o

* Decis√£o:
  Tirar per√≠odo Janeiro-2023 da raw, dada inconsist√™ncia dos dados

* Evolu√ß√µes:
  * Tabela de par√¢metros, tornando ingest√µes din√¢micas (flag para ingerir ou n√£o)
  * Testes de integra√ß√£o, unit√°rios
  * Estrat√©gia SCD 