# Boas vindas ao **ifood-case!**

Para executar o projeto, observe as orienta√ß√µes descritas a seguir, e se tiver qualquer d√∫vida, sugest√£o, contribui√ß√£o, considere abrir uma issue ou entrar em contato. üöÄ

Aqui voc√™ vai encontrar os detalhes de como est√° estruturado e foi desenvolvido o projeto.

# <a id='topicos'>T√≥picos</a>
- [Desenvolvimento](#desenvolvimento)
  - [Objetivo](#objetivo)
  - [Estrutura do projeto](#estrutura)
  - [Tecnologias utilizadas](#tecnologias)
- [Orienta√ß√µes](#orientacoes)
  - [Executando o projeto](#execucao)
    - [Requisitos](#requisitos)
    - [Configura√ß√µes Necess√°rias](#settings)
    - [Executando Setup](#setup)
    - [Executando Jobs Databricks](#jobs)
- [Implementa√ß√µes](#implementacoes)
  - [Contextualizando](#contextualizando)
  - [Continuous Delivery](#ci)
    - [NYC Bucket Setup](#bs)
    - [Databricks Setup](#db)
  - [Camada de Consumo](#cl)
    - [Desenho do ambiente](#layers)
    - [Modelagem de Dados](#der)
  - [Tagueamento do ambiente](#tags)
  - [DataOps](#dataops)
- [Decis√µes Arquiteturais](#adr)
  - [Defini√ß√µes de Solu√ß√£o](#c4-model)
  - [Registros de Decis√£o](#registros)
- [Pr√≥ximos passos](#next)

# <a id='desenvolvimento'>[Desenvolvimento](#topicos)</a>

<strong><a id='objetivo'>[Objetivo](#topicos)</a></strong>

  O **objetivo** √© construir um ambiente de desenvolvimento de um datalake com os dados da Taxi & Limousine Comission (TLC) da cidade de Nova York, dispon√≠veis [aqui](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Em um primeiro recorte, com o sample de Janeiro a Maio de 2023.
  
  Para isso, foi definida uma [**arquitetura de refer√™ncia**](#c4-model) e [**modelagem dos dados**](#der) tornando a disponibilidade e consumo do ambiente anal√≠tico escal√°vel e resiliente.

  ---

<strong><a id='estrutura'>[Estrutura do projeto](#topicos)</a></strong>

* **Na pasta [.github](.github) est√£o os diret√≥rios:**
  * **[actions](.github\actions)** com custom actions do `GitHub Actions`, modularizando e desaclopando steps dos workflows de subida dos componentes da arquitetura de refer√™ncia;
  * **[workflows](.github\workflows)** com os workflows `GitHub Actions` que iniciam o setup das duas solu√ß√µes adotadas para constru√ß√£o do datalake para o projeto:
    * **[databricks_setup](.github\workflows\databricks_setup.yml)** que adiciona as secrets, esse reposit√≥rio e cria no ambiente Databricks informado, os jobs de ingest√£o e transforma√ß√£o de dados da TLC;
    * **[nyc_bucket_setup](.github\workflows\nyc_bucket_setup.yml)** que cria um bucket S3 no console AWS informado, para storage do datalake pavimentado pelo Databricks;
* **Na pasta [analysis](analysis) est√£o os arquivos que endere√ßam quest√µes de neg√≥cio sobre os dados do TLC NYC**;
* **Na pasta [devops](devops) est√£o os m√≥dulos utilizados pelos workflows `GitHub Actions`** para pavimenta√ß√£o do Databricks e S3 (baseado em Terraform);
* **Na pasta [src](src) est√£o os diret√≥rios:**
  * **[dataops](src\dataops)** com os c√≥digos fonte utilizados para as opera√ß√µes do ambiente anal√≠tico, atualmente no processamento de dados espec√≠ficos da origem que n√£o puderam seguir o fluxo normal. Mais sobre o tema [aqui](#registros);
  * **[ingestion](src\ingestion)** com os c√≥digos fonte do job de ingest√£o dos dados do TLC para a camada bruta (raw) de processamento;
  * **[transform](src\transform)** com os c√≥digos fonte do job de transforma√ß√µes dos dados brutos e pouso nas camadas de consumo dos times de an√°lise (refined e trusted);

  ---

<strong><a id='tecnologias'>[Tecnologias utilizadas](#topicos)</a></strong>

  O projeto foi desenvolvido utilizando o AWS S3 como solu√ß√£o de armazenamento dos dados, e o Databricks Free Edition como solu√ß√£o para aquisi√ß√£o, processamento e consumo (serving) no ambiente anal√≠tico.

  Para provisionamento do c√≥digo, o `Github Actions` foi a solu√ß√£o utilizada para entrega e integra√ß√£o cont√≠nua da infraestrutura e desenvolvimento do ambiente. E o `Terraform` foi a op√ß√£o empregada para versionamento e deploy da infraestrutura integral do S3.

  Ainda sobre o processamento dos dados, o `Python` foi utilizado como API principal, e as bibliotecas e engines utilizadas atrav√©s do Databricks, baseadas no Python, foram as abaixo:

  * **[boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html):** Kit de desenvolvimento (SDK) da AWS que habilita via c√≥digo ter uma interface program√°tica com os recursos da cloud. Aqui utilizada como solu√ß√£o de aquisi√ß√£o de dados, consumindo da origem e copiando para o storage.
  * **[PySpark](https://spark.apache.org/docs/latest/api/python/index.html):** API em python do `Apache Spark`, que habilita processamento de dados massivos (Big Data) utilizando a linguagem e tamb√©m o SQL. 
  √â tamb√©m solu√ß√£o prim√°ria no Databricks, que opera via Cluster Jobs do Spark toda a carga e recursos de rede que utiliza.

  ---

# <a id='orientacoes'>[Orienta√ß√µes](#topicos)</a>

<strong><a id='execucao'>[Executando o projeto](#topicos)</a></strong>

  O projeto foi pensado para ser reproduz√≠vel com uma conta na AWS e Databricks Free Edition. Detalhes da escolha das solu√ß√µes [aqui](#adr)

  Importante seguir o passo a passo abaixo para execu√ß√£o dele

### <strong><a id='requisitos'>[1. Requisitos:](#topicos)</a></strong>

>**IMPORTANTE**<br/>Observe os padr√µes de nomes dos recursos e credenciais, para integra√ß√£o e funcionamento correto do c√≥digo e ambientes utilizados.

* Fazer fork desse projeto em sua conta Github
* Conta AWS
* Conta Databricks Free Edition

### <strong><a id='settings'>[2. Configura√ß√µes necess√°rias:](#topicos)</a></strong>

* Par√¢metros/Credenciais a criar:
  * AWS Console:
    * Access Key (Chave de Acesso). Tutorial [aqui](https://docs.aws.amazon.com/keyspaces/latest/devguide/create.keypair.html).
    * Instance Provider (Provedor de Inst√¢ncia) para integra√ß√£o AWS <> GitHub Actions

      <details>        
        <summary><strong>Passo a passo</strong></summary><br />

        * Criar OIDC (OpenID Connect):
          * Refer√™ncia [aqui](https://aws.amazon.com/pt/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/)
          * Necess√°rio:
            * Informar nome do seu perfil no campo `Github Organization` , na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
            * Coloque `NycTripRecordOidcRole` como nome da role na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
            * Selecione a role: `AmazonS3FullAccess` na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
          * Recomendado:
            * Informar url do reposit√≥rio que foi feito o fork, na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima
            * Colocar tags desse projeto;
            * Especificar branch do reposit√≥rio que foi feito o fork, na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima
        * Adicionar `ARN` do Identity Provider criado nas vari√°veis do reposit√≥rio criado
          * No seu reposit√≥rio acesse:
            * Aba `Settings` 
            * Na se√ß√£o `Security`, clique em `Secrets and variables` 
            *Clique em `Actions` 
            * Na sequ√™ncia, clique na aba `Variables` 
            * Crie uma vari√°vel com o nome `NYC_TRIP_RECORD_OIDC_ARN`, com o valor do ARN do Identity Provider criado
            * Crie uma vari√°vel com o nome `NYC_TRIP_RECORD_AWS_REGION` com o valor us-east-2
          * **IMPORTANTE:** os nomes acima e a cria√ß√£o dessas vari√°veis √© necess√°rio para deploy do S3 que utiliza essas configura√ß√µes
          * Role para o Instance Provider com o nome `NycTripRecordOidcRole` 
            > IMPORTANTE: √â necess√°rio usar esse nome para funcionamento da cria√ß√£o do S3
        
        ---

        </details>

  * Databricks:
    * Personal Access Token (PAT) do Databricks. Tutorial [aqui](https://docs.databricks.com/aws/pt/dev-tools/auth/pat#databricks-acesso-pessoal-para-usu%C3%A1rios-tokens-workspace)
  * Github:
    * Repository Secrets: (Tutorial [aqui](https://docs.github.com/pt/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets#creating-secrets-for-a-repository))
      * AWS_ACCESS_KEY_ID: Com esse nome, e valor da access key da AWS criada
      * AWS_SECRET_ACCESS_KEY: Com esse nome, e valor da access key secret da AWS criada
      * DATABRICKS_TOKEN: Com esse nome, e valor do PAT Databricks
      * NYC_TRIP_RECORD_OIDC_ROLE_ARN: Com esse nome, e valor do ARN da Role do Instance Provider criada

### <strong><a id='setup'>[3. Executando Setup:](#topicos)</a></strong>

A cria√ß√£o do ambiente √© executada com o Github Actions, dividida em dois fluxos:

- Setup S3 (cria√ß√£o do bucket e diret√≥rios das camadas de processamento)
- Setup Databricks (cria√ß√£o das secrets AWS no Databricks, clone do reposit√≥rio e cria√ß√£o dos jobs na workspace)

Foi definida a execu√ß√£o manual dos workflows pensando na seguran√ßa e resili√™ncia dos ambientes, com a evolu√ß√£o dos recursos feita p√≥s setup 

Par√¢metros a informar na execu√ß√£o dos workflows:

  * Workflow Databricks Setup:
    * **databricks_host**: Url do Databricks antes do par√¢metro "?o=<n√∫mero_workspace>
      * Ex: url: 
        * https://dbc-ab3dba61-89cc.cloud.databricks.com/?o=3912183202474156; 
        * databricks_host: https://dbc-ab3dba61-89cc.cloud.databricks.com/
    * **databricks_user_email**: Seu email utilizado para login no Workspace Databricks
  * Workflow NYC Trip Record S3 Setup:
    * **environment**: Com os valores 'dev' ou 'prod' (a ser implementado)
    * **aws_region**: Com o valor da regi√£o a criar o bucket S3
      > IMPORTANTE: Para a vers√£o free do Databricks, usar a regi√£o us-east-2 para funcionamento da integra√ß√£o AWS <> Databricks

### <strong><a id='jobs'>[4. Executando Jobs Databricks:](#topicos)</a></strong>

Ap√≥s execu√ß√£o dos workflows de setup, os jobs abaixo devem aparecer na workspace Databricks informada.

Para constru√ß√£o da camada de consumo de dados, execute eles nessa sequ√™ncia: (Como executar um job Databricks [aqui](https://docs.databricks.com/aws/pt/jobs/run-now#with-different-params))

* **nyc_trip_record_ingestion**: faz a c√≥pia dos dados do TLC NYC, para a camada bruta do bucket S3 dedicado
  * par√¢metros a informar:
    * car_type:
      * para mais de um separar nomes por v√≠rgula sem espa√ßos. 
        * Ex: yellow,green    
      * valores poss√≠veis:
        * yellow
        * green
        * fhv
        * fhvhv
        * all (para buscar dados de todos os tipos)
    * years
      * para mais de um separar nomes por v√≠rgula sem espa√ßos. 
      * Ex: 2023,2024
    * months:  
      * para mais de um, seguir o padr√£o <m√™s de in√≠cio>-<m√™s final>. 
        * ex: janeiro a maio: 1-5

* **nyc_trip_record_refined_load**: faz a carga dos dados ingeridos na raw para a camada refined, tendo como escopo:
  * Selecionar colunas necess√°rias para a camada trusted
  * Deduplicar os dados

* **nyc_trip_record_trusted_load**: faz a carga dos dados da camada silver para a camada trusted, atualizando a tabela fato, e recriando as dimens√µes. Detalhes da modelagem dimensional [aqui](#der)

Definiu-se a execu√ß√£o manual dos jobs considerando economia dos recursos em cloud e a id√©ia de simular ambiente de desenvolvimento com o projeto.

Como d√©bito t√©cnico, a implementa√ß√£o de CDC ser√° constru√≠da para as camadas refined e trusted.

  ---

# <a id='implementacoes'>[Implementa√ß√µes](#topicos)</a>

<strong><a id='contextualizando'>[Contextualizando](#topicos)</a></strong>

Os dados da origem s√£o arquivos parquet com o registro de corridas mensal das empresas de taxi e servi√ßos de plataforma (Uber, Lift, ...) na cidade de Nova York.

Foi sugest√£o da solicitante o uso da AWS para storage dos dados processados, e Databricks para constru√ß√£o das camadas de processamento. 

Os requisitos levantados foram: 
* O per√≠odo de Janeiro a Maio de 2023 para piloto do ambiente;
* A disponibiliza√ß√£o das colunas abaixo na camada de consumo:
  * **VendorID**
  * **passenger_count** 
  * **total_amount**
  * **tpep_pickup_datetime** 
  * **tpep_dropoff_datetime**
* O direcionamento das seguintes perguntas sobre o neg√≥cio:
  * Qual a m√©dia de passageiros (passenger\_count) por cada hora do dia que pegaram t√°xi no m√™s de maio considerando todos os t√°xis da frota?
  * Qual a m√©dia de valor total (total\_amount) recebido em um m√™s considerando todos os yellow t√°xis da frota?

O funcionamento do projeto √© apresentado abaixo:

<strong><a id='ci'>[Continuous Delivery](#topicos)</a></strong>

O Setup do ambiente foi pensado para automatizar a replica√ß√£o do ambiente e prova de valor do projeto.

Dois workflows Github Actions comp√µem o fluxo de entrega:

<strong><a id='bs'>[NYC Bucket Setup](#topicos)</a></strong>

<strong><a id='db'>[Databricks Setup](#topicos)</a></strong>

<strong><a id='cl'>[Camada de Consumo](#topicos)</a></strong>

<strong><a id='layers'>[Desenho do ambiente](#topicos)</a></strong>

<strong><a id='der'>[Modelagem de Dados](#topicos)</a></strong>

<strong><a id='tags'>[Tagueamento do ambiente](#topicos)</a></strong>

<strong><a id='dataops'>[DataOps](#topicos)</a></strong>

  ---

# <a id='adr'>[Decis√µes Arquiteturais](#topicos)</a>

<strong><a id='c4-model'>[Defini√ß√µes de Solu√ß√£o](#topicos)</a></strong>

<strong><a id='registros'>[Registros de Decis√£o](#topicos)</a></strong>


  ---

# <a id='next'>[Pr√≥ximos passos](#topicos)</a>


* Configurar conex√£o AWS<>Databricks Free Edition 
  * Limita√ß√µes (para ADR): Instance profile precisa de databricks provisionado na AWS: https://docs.databricks.com/aws/pt/connect/storage/tutorial-s3-instance-profile
  * Storage credential e external location tem restri√ß√µes tambem. Cria√ß√£o delas foi feita com sucesso atrav√©s do AWS Quickstart (que utiliza CloudFormation)
  * Decis√£o: criar access_key e access_secret_key no Console AWS e colocar em secrets github


* Configurar ingest√£o
  * Documentar CloudFormation
  * Execu√ß√£o dos jobs de ingest√£o

* Decis√£o:
  Tirar per√≠odo Janeiro-2023 da raw, dada inconsist√™ncia dos dados

* Evolu√ß√µes:
  * Tabela de par√¢metros, tornando ingest√µes din√¢micas (flag para ingerir ou n√£o)
  * Testes de integra√ß√£o, unit√°rios
  * Estrat√©gia SCD 