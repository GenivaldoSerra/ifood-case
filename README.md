# Boas vindas ao **NYC Trip Record!**

Para executar o projeto, observe as orienta√ß√µes descritas a seguir, e se tiver qualquer d√∫vida, sugest√£o, contribui√ß√£o, considere abrir uma issue ou entrar em contato. üöÄ

Aqui voc√™ vai encontrar os detalhes de como est√° estruturado e foi desenvolvido o projeto.

# <a id='topicos'>T√≥picos</a>
- [Desenvolvimento](#desenvolvimento)
  - [Objetivo](#objetivo)
  - [Estrutura do projeto](#estrutura)
  - [Tecnologias utilizadas](#tecnologias)
- [Orienta√ß√µes](#orientacoes)
  - [Executando o projeto](#execucao)
    - [Requisitos](#requisitos)
    - [Configura√ß√µes Necess√°rias](#settings)
    - [Executando Setup](#setup)
    - [Configurando External Location](#external)
    - [Executando Jobs Databricks](#jobs)
- [Implementa√ß√µes](#implementacoes)
  - [Contextualizando](#contextualizando)
  - [Continuous Delivery](#ci)
    - [NYC Bucket Setup](#bs)
    - [Databricks Setup](#db)
  - [Camada de Consumo](#cl)
    - [Desenho do ambiente](#layers)
    - [Modelagem de Dados](#der)
  - [Tagueamento do ambiente](#tags)
  - [DataOps](#dataops)
- [Decis√µes Arquiteturais](#adr)
  - [Defini√ß√µes de Solu√ß√£o](#c4-model)
  - [Registros de Decis√£o](#registros)
- [Pr√≥ximos passos](#next)

# <a id='desenvolvimento'>[Desenvolvimento](#topicos)</a>

<strong><a id='objetivo'>[Objetivo](#topicos)</a></strong>

  O **objetivo** √© construir um ambiente de desenvolvimento de um datalake com os dados da Taxi & Limousine Comission (TLC) da cidade de Nova York, dispon√≠veis [aqui](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Em um primeiro recorte, com o sample de Janeiro a Maio de 2023.
  
  Para isso, foi definida uma [**arquitetura de refer√™ncia**](#c4-model) e [**modelagem dos dados**](#der) tornando a disponibilidade e consumo do ambiente anal√≠tico escal√°vel e resiliente.

  ---

<strong><a id='estrutura'>[Estrutura do projeto](#topicos)</a></strong>

* **Na pasta [.github](.github) est√£o os diret√≥rios:**
  * **[actions](.github\actions)** com custom actions do `GitHub Actions`, modularizando e desaclopando steps dos workflows de subida dos componentes da arquitetura de refer√™ncia;
  * **[workflows](.github\workflows)** com os workflows `GitHub Actions` que iniciam o setup das duas solu√ß√µes adotadas para constru√ß√£o do datalake para o projeto:
    * **[databricks_setup](.github\workflows\databricks_setup.yml)** que adiciona as secrets, esse reposit√≥rio e cria no ambiente Databricks informado, os jobs de ingest√£o e transforma√ß√£o de dados da TLC;
    * **[nyc_bucket_setup](.github\workflows\nyc_bucket_setup.yml)** que cria um bucket S3 no console AWS informado, para storage do datalake pavimentado pelo Databricks;
* **Na pasta [analysis](analysis) est√£o os arquivos que endere√ßam quest√µes de neg√≥cio sobre os dados do TLC NYC**;
* **Na pasta [devops](devops) est√£o os m√≥dulos utilizados pelos workflows `GitHub Actions`** para pavimenta√ß√£o do Databricks e S3 (baseado em Terraform);
* **Na pasta [src](src) est√£o os diret√≥rios:**
  * **[dataops](src\dataops)** com os c√≥digos fonte utilizados para as opera√ß√µes do ambiente anal√≠tico, atualmente no processamento de dados espec√≠ficos da origem que n√£o puderam seguir o fluxo normal. Mais sobre o tema [aqui](#registros);
  * **[ingestion](src\ingestion)** com os c√≥digos fonte do job de ingest√£o dos dados do TLC para a camada bruta (raw) de processamento;
  * **[transform](src\transform)** com os c√≥digos fonte do job de transforma√ß√µes dos dados brutos e pouso nas camadas de consumo dos times de an√°lise (refined e trusted);

  ---

<strong><a id='tecnologias'>[Tecnologias utilizadas](#topicos)</a></strong>

  O projeto foi desenvolvido utilizando o AWS S3 como solu√ß√£o de armazenamento dos dados, e o Databricks Free Edition como solu√ß√£o para aquisi√ß√£o, processamento e consumo (serving) no ambiente anal√≠tico.

  Para provisionamento do c√≥digo, o `Github Actions` foi a solu√ß√£o utilizada para entrega e integra√ß√£o cont√≠nua da infraestrutura e desenvolvimento do ambiente. E o `Terraform` foi a op√ß√£o empregada para versionamento e deploy da infraestrutura integral do S3.

  Ainda sobre o processamento dos dados, o `Python` foi utilizado como API principal, e as bibliotecas e engines utilizadas atrav√©s do Databricks, baseadas no Python, foram as abaixo:

  * **[boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html):** Kit de desenvolvimento (SDK) da AWS que habilita via c√≥digo ter uma interface program√°tica com os recursos da cloud. Aqui utilizada como solu√ß√£o de aquisi√ß√£o de dados, consumindo da origem e copiando para o storage.
  * **[PySpark](https://spark.apache.org/docs/latest/api/python/index.html):** API em python do `Apache Spark`, que habilita processamento de dados massivos (Big Data) utilizando a linguagem e tamb√©m o SQL. 
  √â tamb√©m solu√ß√£o prim√°ria no Databricks, que opera via Cluster Jobs do Spark toda a carga e recursos de rede que utiliza.

  ---

# <a id='orientacoes'>[Orienta√ß√µes](#topicos)</a>

<strong><a id='execucao'>[Executando o projeto](#topicos)</a></strong>

  O projeto foi pensado para ser reproduz√≠vel com uma conta na AWS e Databricks Free Edition. Detalhes da escolha das solu√ß√µes [aqui](#adr)

  Importante seguir o passo a passo abaixo para execu√ß√£o dele

### <strong><a id='requisitos'>[1. Requisitos:](#topicos)</a></strong>

>**IMPORTANTE**<br/>Observe os padr√µes de nomes dos recursos e credenciais, para integra√ß√£o e funcionamento correto do c√≥digo e ambientes utilizados.

* Fazer fork desse projeto em sua conta Github
* Conta AWS
* Conta Databricks Free Edition

### <strong><a id='settings'>[2. Configura√ß√µes necess√°rias:](#topicos)</a></strong>

* Par√¢metros/Credenciais a criar:
  * AWS Console:
    * Access Key (Chave de Acesso). Tutorial [aqui](https://docs.aws.amazon.com/keyspaces/latest/devguide/create.keypair.html).
    * Instance Provider (Provedor de Inst√¢ncia) para integra√ß√£o AWS <> GitHub Actions

      <details>        
        <summary><strong>Passo a passo</strong></summary><br />

        * Criar OIDC (OpenID Connect):
          * Refer√™ncia [aqui](https://aws.amazon.com/pt/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/)
          * Necess√°rio:
            * Informar nome do seu perfil no campo `Github Organization` , na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
            * Coloque `NycTripRecordOidcRole` como nome da role na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
            * Selecione a role: `AmazonS3FullAccess` na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima (Step 2);
          * Recomendado:
            * Informar url do reposit√≥rio que foi feito o fork, na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima
            * Colocar tags desse projeto;
            * Especificar branch do reposit√≥rio que foi feito o fork, na cria√ß√£o da fun√ß√£o (role) do OIDC, conforme a refer√™ncia acima
        * Adicionar `ARN` do Identity Provider criado nas vari√°veis do reposit√≥rio criado
          * No seu reposit√≥rio acesse:
            * Aba `Settings` 
            * Na se√ß√£o `Security`, clique em `Secrets and variables` 
            *Clique em `Actions` 
            * Na sequ√™ncia, clique na aba `Variables` 
            * Crie uma vari√°vel com o nome `NYC_TRIP_RECORD_OIDC_ARN`, com o valor do ARN do Identity Provider criado
            * Crie uma vari√°vel com o nome `NYC_TRIP_RECORD_AWS_REGION` com o valor us-east-2
          * **IMPORTANTE:** os nomes acima e a cria√ß√£o dessas vari√°veis √© necess√°rio para deploy do S3 que utiliza essas configura√ß√µes
          * Role para o Instance Provider com o nome `NycTripRecordOidcRole` 
            > IMPORTANTE: √â necess√°rio usar esse nome para funcionamento da cria√ß√£o do S3
        
        ---

        </details>

  * Databricks:
    * Personal Access Token (PAT) do Databricks. Tutorial [aqui](https://docs.databricks.com/aws/pt/dev-tools/auth/pat#databricks-acesso-pessoal-para-usu%C3%A1rios-tokens-workspace)
  * Github:
    * Repository Secrets: (Tutorial [aqui](https://docs.github.com/pt/actions/how-tos/write-workflows/choose-what-workflows-do/use-secrets#creating-secrets-for-a-repository))
      * AWS_ACCESS_KEY_ID: Com esse nome, e valor da access key da AWS criada
      * AWS_SECRET_ACCESS_KEY: Com esse nome, e valor da access key secret da AWS criada
      * DATABRICKS_TOKEN: Com esse nome, e valor do PAT Databricks
      * NYC_TRIP_RECORD_OIDC_ROLE_ARN: Com esse nome, e valor do ARN da Role do Instance Provider criada

### <strong><a id='setup'>[3. Executando Setup:](#topicos)</a></strong>

A cria√ß√£o do ambiente √© executada com o Github Actions, dividida em dois fluxos:

- Setup S3 (cria√ß√£o do bucket e diret√≥rios das camadas de processamento)
- Setup Databricks (cria√ß√£o das secrets AWS no Databricks, clone do reposit√≥rio e cria√ß√£o dos jobs na workspace)

Foi definida a execu√ß√£o manual dos workflows pensando na seguran√ßa e resili√™ncia dos ambientes, com a evolu√ß√£o dos recursos feita p√≥s setup 

Par√¢metros a informar na execu√ß√£o dos workflows:

  * Workflow Databricks Setup:
    * **databricks_host**: Url do Databricks antes do par√¢metro "?o=<n√∫mero_workspace>
      * Ex: url: 
        * https://dbc-ab3dba61-89cc.cloud.databricks.com/?o=3912183202474156; 
        * databricks_host: https://dbc-ab3dba61-89cc.cloud.databricks.com/
    * **databricks_user_email**: Seu email utilizado para login no Workspace Databricks
  * Workflow NYC Trip Record S3 Setup:
    * **environment**: Com os valores 'dev' ou 'prod' (a ser implementado)
    * **aws_region**: Com o valor da regi√£o a criar o bucket S3
      > IMPORTANTE: Para a vers√£o free do Databricks, usar a regi√£o us-east-2 para funcionamento da integra√ß√£o AWS <> Databricks

### <strong><a id='external'>[4. Configurando External Location via Databricks GUI](#topicos)</a></strong>

Para o Databricks Free Edition a cria√ß√£o da external location e storage credential, que permitem a conex√£o do Databricks com o Bucket S3, s√≥ foi poss√≠vel utilizando o AWS CloudFormation Quickstart.

Siga [esse tutorial](https://docs.databricks.com/aws/en/connect/unity-catalog/cloud-storage/s3/s3-external-location-cfn#step-1-create-an-external-location-for-an-s3-bucket-using-an-aws-cloudformation-template) antes de sequenciar as demais etapas de execu√ß√£o do projeto.

>IMPORTANTE: No setup do AWS Quickstart, utilize essa url como caminho do seu bucket: `s3://nyc-trip-record-ifood/`

>IMPORTANTE: √â poss√≠vel que o nome do bucket usado nesse projeto esteja em uso durante sua execu√ß√£o. Nesse cen√°rio, modifique o nome do bucket utilizado nesses arquivos e na url apresentada anteriormente:
* nome atual: nyc-trip-record-ifood
* devops/terraform/_variables.tf
* src/ingestion/ingestion.py

### <strong><a id='jobs'>[5. Executando Jobs Databricks:](#topicos)</a></strong>

Ap√≥s execu√ß√£o dos workflows de setup, os jobs abaixo devem aparecer na workspace Databricks informada.

Para constru√ß√£o da camada de consumo de dados, execute eles nessa sequ√™ncia: (Como executar um job Databricks [aqui](https://docs.databricks.com/aws/pt/jobs/run-now#with-different-params))

* **nyc_trip_record_ingestion**: faz a c√≥pia dos dados do TLC NYC, para a camada bruta do bucket S3 dedicado
  * par√¢metros a informar:
    * car_type:
      * para mais de um separar nomes por v√≠rgula sem espa√ßos. 
        * Ex: yellow,green    
      * valores poss√≠veis:
        * yellow
        * green
        * fhv
        * fhvhv
        * all (para buscar dados de todos os tipos)
    * years
      * para mais de um separar nomes por v√≠rgula sem espa√ßos. 
      * Ex: 2023,2024
    * months:  
      * para mais de um, seguir o padr√£o <m√™s de in√≠cio>-<m√™s final>. 
        * ex: janeiro a maio: 1-5

* **nyc_trip_record_refined_load**: faz a carga dos dados ingeridos na raw para a camada refined, tendo como escopo:
  * Selecionar colunas necess√°rias para a camada trusted
  * Deduplicar os dados

* **Reparo da carga de Janeiro de 2023**: Antes de executar a ingest√£o trusted, fa√ßa o reparo previsto [aqui](#dataops).

* **nyc_trip_record_trusted_load**: faz a carga dos dados da camada silver para a camada trusted, atualizando a tabela fato, e recriando as dimens√µes. Detalhes da modelagem dimensional [aqui](#der)

Definiu-se a execu√ß√£o manual dos jobs considerando economia dos recursos em cloud e a id√©ia de simular ambiente de desenvolvimento com o projeto.

Como d√©bito t√©cnico, a implementa√ß√£o de CDC ser√° constru√≠da para as camadas refined e trusted.

  ---

# <a id='implementacoes'>[Implementa√ß√µes](#topicos)</a>

<strong><a id='contextualizando'>[Contextualizando](#topicos)</a></strong>

Os dados da origem s√£o arquivos parquet com o registro de corridas mensal das empresas de taxi e servi√ßos de plataforma (Uber, Lift, ...) na cidade de Nova York.

Foi sugest√£o da solicitante o uso da AWS para storage dos dados processados, e Databricks para constru√ß√£o das camadas de processamento. 

Os requisitos levantados foram: 
* O per√≠odo de Janeiro a Maio de 2023 para piloto do ambiente;
* A disponibiliza√ß√£o das colunas abaixo na camada de consumo:
  * **VendorID**
  * **passenger_count** 
  * **total_amount**
  * **tpep_pickup_datetime** 
  * **tpep_dropoff_datetime**

* O direcionamento das seguintes perguntas sobre o neg√≥cio:
  * Qual a m√©dia de passageiros (passenger\_count) por cada hora do dia que pegaram t√°xi no m√™s de maio considerando todos os t√°xis da frota?
  * Qual a m√©dia de valor total (total\_amount) recebido em um m√™s considerando todos os yellow t√°xis da frota?

O funcionamento do projeto √© apresentado abaixo:

<strong><a id='ci'>[Continuous Delivery](#topicos)</a></strong>

O Setup do ambiente foi pensado para automatizar a replica√ß√£o do ambiente e prova de valor do projeto.

Dois workflows Github Actions comp√µem o fluxo de entrega:

<strong><a id='bs'>[NYC Bucket Setup](#topicos)</a></strong>

Para [execu√ß√£o do setup do S3](#setup), essa interface do GitHub Actions foi implementada.

**Imagem 01 - Execu√ß√£o de Setup do S3.**
![nyc_s3_setup](docs/images/nyc_s3_setup.png)

<strong><a id='db'>[Databricks Setup](#topicos)</a></strong>

Para [execu√ß√£o do setup do Databricks](#setup), essa interface do GitHub Actions foi implementada.

**Imagem 02 - Execu√ß√£o de Setup do Databricks.**
![databricks_setup](docs/images/databricks_setup.png)

<strong><a id='cl'>[Camada de Consumo](#topicos)</a></strong>

A camada de consumo definida foi o Databricks, onde a explora√ß√£o do c√≥digo fonte e desenvolvimento √† partir do lake √© disponibilizada utilizando python, SQL e Spark.

**Imagem 03 - Consumindo camada trusted com Databricks.**
![consuming_layer](docs/images/consuming_layer.png)

<strong><a id='layers'>[Desenho do ambiente](#topicos)</a></strong>

Buscando o tracking e segrega√ß√£o do consumo dos dados, o processamento dos dados foi definido utilizando as seguintes camadas:
* Cat√°logo: nyc_trip_record
* raw
* refined
* trusted

**Imagem 04 - Camadas de processamento no Databricks.**
![layers](docs/images/layers.png)

Com os diferentes n√≠veis de acur√°cia, √† partir da camada refined √© possivel gerar vis√µes de neg√≥cio com base nos requisitos levantados.

E a camada trusted traz a consolida√ß√£o dos dados para cria√ß√£o de produtos de dados e insights refinados sobre as bases

<strong><a id='der'>[Modelagem de Dados](#topicos)</a></strong>

A camada raw traz os dados como na origem (as-is), seguindo o schema original dos dados

A camada refined atualmente s√≥ apresenta os dados de taxis (yellow e green taxi) dada a [demanda de neg√≥cio](#contextualizando) focada nesses ve√≠culos, representados no diagrama abaixo

**Imagem 05 - Diagrama Camada Refined.**
![refined_layer](docs\images\refined_tables.png)

Por fim, a camada trusted persiste os dados seguindo o modelo dimensional, apresentado na imagem 06.

**Imagem 06 - Diagrama Camada Trusted.**
![trusted_layer](docs\images\trusted_tables.png)

Detalhamento das dimens√µes abaixo:

### Tabela dim_trip_time:
|Coluna|Tipo|Descri√ß√£o|
|-|-|-|
|trip_time_id|STRING|Surrougate Key concatenando pickup_datetime e dropoff_datetime|
|pickup_datetime|TIMESTAMP|Timestamp do come√ßo da corrida. Concentrando tpep_pickup_datetime e lpep_pickup_datetime dos dados brutos|
|pickup_month|INTEGER|M√™s do come√ßo da corrida|
|pickup_hour|INTEGER|Hora do come√ßo da corrida|
|dropoff_datetime|TIMESTAMP|Timestamp do fim da corrida. Concentrando tpep_dropoff_datetime e lpep_dropoff_datetime dos dados brutos|
|dropoff_month|INTEGER|M√™s do fim da corrida|
|dropoff_hour|INTEGER|Hora do fim da corrida|
### Tabela dim_vendor:
|Coluna|Tipo|Descri√ß√£o|
|-|-|-|
|vendor_id|STRING|Surrougate Key concatenando vendor_code, vendor_name e car_type|
|vendor_code|BIGINT|VendorID dos dados brutos|
|vendor_name|STRING|Nome do vendo com base no dicion√°rio de dados|
|car_type|STRING|Tipo do carro (yellow, green, fhv, fhvhv)|
### Tabela fact_trip:
|Coluna|Tipo|Descri√ß√£o|
|-|-|-|
|fact_id|STRING|Surrougate Key concatenando vendor_id, pickup_datetime, dropoff_datetime, total_amount e passenger_count|
|vendor_id|STRING|da dim_vendor|
|trip_time_id|STRING|da dim_trip_time|
|passenger_count|LONG|Quantidade de passageiros da corrida|
|total_amount|DOUBLE|Valor pago por corrida|


<strong><a id='tags'>[Tagueamento do ambiente](#topicos)</a></strong>

Buscando a governan√ßa e monitoria do ambiente, os recursos provisionados nascem com as seguintes tags:

* component: setup | data_integration
* cost_center: nyc_trip_record
* developer: calilisantos@gmail.com
* env: dev
* resource: 
    nyc_trip_record_oidc
    nyc_trip_record_oidc_role
    nyc_trip_record_bucket
    nyc_trip_record_ingestion_job
    nyc_trip_record_refined_load_job
    nyc_trip_record_trusted_load_job

<strong><a id='dataops'>[DataOps](#topicos)</a></strong>

Para consolidar as opera√ß√µes do ambiente anal√≠tico, foi feita uma camada staging no bucket, nesse primeiro momento para reparo da ingest√£o dos arquivos de Janeiro de 2023 da base de dados que apresentava inconsist√™ncia de schema.

O c√≥digo de reparo est√° presente [aqui](src\dataops\fix_load.py), com a subida feita de forma manual dos arquivos desse per√≠odo para os taxis green e yellow, com sua carga feita diretamente para a camada refined.

  ---

# <a id='adr'>[Decis√µes Arquiteturais](#topicos)</a>

<strong><a id='c4-model'>[EM CONSTRU√á√ÉO - Defini√ß√µes de Solu√ß√£o](#topicos)</a></strong>

Para defini√ß√µes de arquitetura, escolheu-se o C4-Model como refer√™ncia de modelagem da solu√ß√£o, e a utiliza√ß√£o de registros de arquitetura (ADR) para gest√£o do conhecimento, utilizando o template MADR. 

<strong><a id='registros'>[Registros de Decis√£o](#topicos)</a></strong>

* Arquitetura de Refer√™ncia
  * Inicialmente levantou-se como cen√°rio ideal segregar ingest√£o e processamento dos dados do Databricks, segregando responsabilidades e aproveitando features nativas da cloud para aquisi√ß√£o de dados com algumas solu√ß√µes levantadas (ver esbo√ßos [aqui](docs\drafts.excalidraw) e [aqui](docs\project.archimate))
    * Ingest√£o: 
      * AWS Lambda + EventBridge
      * AWS Step Functions + Glue Job
      * AWS EC2 com Airflow
      * Airflow Gerenciado da AWS (MWAA)
  * Com as limita√ß√µes de configurar o ambiente Databricks Free Edition, e baixa volumetria do sample estabelecido inicialmente, foi definido o uso do boto3 junto com o Databricks para aquisi√ß√£o de dados, decis√£o a ser reconsiderada em ambiente produtivo e evolu√ß√£o do projeto
* Configurar conex√£o AWS<>Databricks Free Edition 
  * Limita√ß√µes: Instance profile precisa de databricks provisionado na AWS: https://docs.databricks.com/aws/pt/connect/storage/tutorial-s3-instance-profile
  * Storage credential e external location tem restri√ß√µes tambem. Cria√ß√£o delas foi feita com sucesso atrav√©s do AWS Quickstart (que utiliza CloudFormation)
  * Decis√£o: criar access_key e access_secret_key no Console AWS e colocar em secrets github
* Feature de Documenta√ß√£o das tabelas com DBRX (Hub de Agentes do Databricks):
  * Acesso de constru√ß√£o do modelo n√£o √© permitida com Databricks Free Edition, postergando feature no momento

  ---

# <a id='next'>[Pr√≥ximos passos](#topicos)</a>

* Rever d√©bitos t√©cnicos:
  * Constru√ß√£o de estrat√©gia de CDC
    * Tabela de par√¢metros, tornando ingest√µes din√¢micas (flag para ingerir ou n√£o)
  * Acoplamento de ingest√£o e transforma√ß√£o no Databricks
* Evolu√ß√µes:
  * Constru√ß√£o de solu√ß√£o de Data Quality para lidar com inconsist√™ncias dos dados
    * Schema dos campos (passenger_count como double em Janeiro de 2023)
    * Dados out-of-sample (2002, 2008, 2014, etc...)
    * Outliers dos valores
  * Utilizar dicion√°rio de dados da origem para documenta√ß√£o das tabelas
  * Testes de integra√ß√£o, unit√°rios
  * Modularizar c√≥digo com strategy e factory patterns
  